Neural Code Summarization: How Far Are We?
Ensheng Shi†§ , Yanlin Wang‡∗ , Lun Du‡ , Junjie Chen†† , Shi Han‡ ,
Hongyu Zhang¶ , Dongmei Zhang‡ and Hongbin Sunk∗
† Xi’an

Jiaotong University, s1530129650@stu.xjtu.edu.cn
Research Asia, {yanlwang, lun.du, shihan, dongmeiz}@microsoft.com
†† Tianjin University, junjiechen@tju.edu.cn
¶ The University of Newcastle, hongyu.zhang@newcastle.edu.au
k Xi’an Jiaotong University, hsun@mail.xjtu.edu.cn

arXiv:2107.07112v1 [cs.SE] 15 Jul 2021

‡ Microsoft

Abstract—Source code summaries are important for the comprehension and maintenance of programs. However, there are
plenty of programs with missing, outdated, or mismatched
summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code
snippets. To achieve a profound understanding of how far we
are from solving this problem, in this paper, we conduct a
systematic and in-depth analysis of five state-of-the-art neural
source code summarization models on three widely used datasets.
Our evaluation results suggest that: (1) The BLEU metric, which
is widely used by existing work for evaluating the performance
of the summarization models, has many variants. Ignoring the
differences among the BLEU variants could affect the validity
of the claimed results. Furthermore, we discover an important,
previously unknown bug about BLEU calculation in a commonlyused software package. (2) Code pre-processing choices can have
a large impact on the summarization performance, therefore
they should not be ignored. (3) Some important characteristics
of datasets (corpus size, data splitting method, and duplication
ratio) have a significant impact on model evaluation. Based on
the experimental results, we give some actionable guidelines on
more systematic ways for evaluating code summarization and
choosing the best method in different scenarios. We also suggest
possible future research directions. We believe that our results
can be of great help for practitioners and researchers in this
interesting area.
Index Terms—Code summarization, Empirical study, Deep
learning, Evaluation

I. I NTRODUCTION
Source code summaries are important for program comprehension and maintenance since developers can quickly
understand a piece of code by reading its natural language
description. However, documenting code with summaries remains a labor-intensive and time-consuming task. As a result,
code summaries are often mismatched, missing, or outdated
in many projects [1]–[3]. Therefore, automatic generation of
code summaries is desirable and many approaches have been
proposed over the years [4]–[16]. Recently, deep learning
(DL) based models are exploited to generate better natural
language summaries for code snippets [9]–[16]. These models
usually adopt a neural machine translation framework to learn
the alignment between code and summaries. Some studies
also enhance DL-based models by incorporating information
retrieval techniques [15], [17]. Generally, the existing neural
source code summarization models show promising results
§ Work performed during the internship at Microsoft Research Asia.
∗ Corresponding Author.

on public datasets and claim their superiority over traditional
approaches.
However, we notice that in the current code summarization
work, there are many important details that could be easily
overlooked and important issues that have not received much
attention. These details and issues are associated with evaluation metrics, experimental datasets, and experimental settings.
In this work, we would like to dive deep into the problem
and answer: how to evaluate and compare the source code
summarization models more correctly and comprehensively?
To answer the above question, we conduct systematic
experiments of five representative source code summarization approaches (including CodeNN [9], Deepcom [10], Astattgru [14], Rencos [15] and NCS [16]) on three widely
used datasets (including TL-CodeSum [11], Funcom [14], and
CodeSearchNet [18]), under controlled experimental settings.
We choose the five approaches with the consideration of
representativeness and diversity. CodeNN is one of the first
DL-based (RNN) models and utilizes code token sequence.
Deepcom captures the syntactic and structural information
from AST. Astattgru uses both code token sequence and AST.
NCS is the first attempt to replace the previous RNN units with
the more advanced Transformer model. Rencos is a representative model that combines information retrieval techniques
with the generation model in the code summarization task.
Our experiments can be divided into three major parts.
We first conduct an in-depth analysis of the BLEU metric,
which is widely used in related work [9]–[17], [19]–[22] (Section IV-A). Then, we explore the impact of different code preprocessing operations (such as token splitting, replacement,
filtering, lowercase) on the performance of code summarization (Section IV-B). Finally, we conduct extensive experiments
on the three datasets from three perspectives: corpus sizes, data
splitting ways, and duplication ratios (Section IV-C).
Through extensive experimental evaluation, we obtain the
following major findings about the current neural code summarization models:
• First, we find that there is a wide variety of BLEU metrics
used in prior work and they produce rather different
results for the same generated summary. At the same
time, we notice that many existing studies simply cite
the original paper of BLEU [23] without explaining
their exact implementation. What’s worse, some software
packages used for calculating BLEU is buggy: 1 They

•

•

may produce a BLEU score greater than 100% (or even
> 700%), which extremely exaggerates the performance
of code summarization models, and 2 the results are also
different across different package versions. Therefore,
some studies may overestimate their model performance
or may fail to achieve fair comparisons, even though
they are evaluated on the same dataset with the same
experimental setting.We further give some suggestions
about the BLEU usage in Section IV-A.
Second, we find that different code pre-processing operations can affect the overall performance by a noticeable
margin of -18% to +25%. Therefore, code pre-processing
should be considered carefully during model evaluation.
We also give suggestions on the choice of data preprocessing operations in Section IV-B.
Third, we find that the code summarization approaches
perform inconsistently on different datasets. For instance,
one approach may perform better than other approaches
on one dataset and poorly on another dataset. Furthermore, three dataset attributes (corpus sizes, data splitting
ways, and duplication ratios) have an important impact
on the performance of code summarization models. For
corpus size, as the size of the training set becomes larger,
the performance of all models will improve. For data
splitting ways, all approaches perform poorly on the
dataset split by project (the same project can only exist
in one partition: train, validation, or test set) than by
method (randomly split dataset). That is, approaches only
tested with datasets split by method may have the risk
of generalization to new projects. For duplication ratios,
we find when the duplication ratio increases, the BLEU
scores of all approaches will increase, but the ranking
among these approaches cannot be preserved.
We further give some suggestions about evaluation
datasets in Section IV-C.

In summary, our findings indicate that in order to evaluate
and compare code summarization models more correctly and
comprehensively, we need to pay much attention to the implementation of BLEU metrics, the way of data pre-processing,
and the usage of datasets.
Several previous surveys and empirical studies on code
summarization are related to our work. For example, some
surveys [24]–[26] provided a taxonomy of code summarization methods and discussed the advantages, limitations, and
challenges of existing models from a high-level perspective.
Song et al. [25] also provided an overview of the evaluation
techniques being used in existing methods. Gros et al. [27]
described an analysis of several machine learning approaches
originally designed for the task of natural language translation
for the code summarization task. They observed that different
datasets were used in existing work and different metrics were
used to evaluate different approaches. Our work differs from
previous work in that we not only observe the inconsistent
usage of different BLEU metrics but also conduct dozens
of experiments on the five models and explicitly confirm

that the inconsistent usage can cause severe problems in
evaluating/comparing models. Moreover, we explore factors
affecting model evaluation, which have not been systematically
studied before, such as dataset size, dataset split methods,
data pre-processing operations, etc. Different from the surveys,
we provide extensive experiments on various datasets for
various findings and corresponding discussions. Finally, we
consolidate all findings and propose guidelines for evaluating
code summarization models.
The major contributions of this work are as follows:
• We conduct an extensive evaluation of five representative neural code summarization models, with different
data pre-processing techniques, evaluation metrics, and
datasets.
• We conclude that many existing code summarization
models are not evaluated comprehensively and do not
generalize well in new experimental settings. Therefore,
more research is needed to further improve code summarization models.
• Based on the evaluation results, we give actionable suggestions for evaluating code summarization models from
multiple perspectives.
II. BACKGROUND
A. Code Summarization
Code summaries are short natural language descriptions of
code snippets that can help developers better understand and
maintain source code. However, in many software projects,
code summaries are often absent or outdated. It arouses
the interests of many researchers to automatically generate
code summaries. In the early stage of automatic source code
summarization, template-based approaches [4], [28]–[31] are
widely used. However, a well-designed template requires expert domain knowledge. Therefore, information retrieval (IR)
based approaches [28]–[31] are proposed. The basic idea of
the IR-based approach is to retrieve terms from source code
to generate term-based summaries or to retrieve similar source
code and use its summary as the target summary. However, the
retrieved summaries may not correctly describe the semantics
and behavior of code snippets, leading to the mismatches
between code and summaries.
Recently, Neural Machine Translation (NMT) based models
are exploited to generate summaries for code snippets [9]–
[14], [16], [19]–[22], [32]–[39]. CodeNN [9] is an early
attempt that uses only code token sequences, followed by
various approaches that utilize AST [10], [13], [14], [19],
[22], [35], API knowledge [11], type information [33], global
context [34], [38], reinforcement learning [12], [40], multi-task
and dual learning [20], [36], [39], and pre-trained language
models [21].
Hybrid approaches [15], [17] that combines the NMTbased and IR-based methods are proposed and shown to be
promising. For instance, Rencos proposed by Zhang et al. [15]
obtains two most similar code snippets based on the syntaxlevel and semantics-level information of the source code, and

feeds the original code and the two retrieved code snippets to
the model to generate summaries. Re2 Com proposed by Wei
et al. [17] is an exemplar-based summary generation method
that retrieves a similar code snippet and summary pair from
the corpus and then utilizes the seq2seq neural network to
modify the summaries.

TABLE I
I NTERPRETATION OF BLEU SCORES [42].

Score

Interpretation

<10
10-19
20-29

Almost useless
Hard to get the gist
The gist is clear, but has significant
grammatical errors
Understandable to good translations
High quality translations
Very high quality, adequate, and fluent translations
Quality often better than human

B. BLEU
Bilingual Evaluation Understudy (BLEU) [23] is commonly
used for evaluating the quality of the generated code summaries [9]–[17], [19]–[22], [38], [39]. In short, a BLEU score
is a percentage number between 0 and 100 that measures the
similarity between one sentence to a set of reference sentences
using constituent n-grams precision scores. BLEU typically
uses BLEU-1, BLEU-2, BLEU-3, and BLEU-4 (calculated by
1-gram, 2-gram, 3-gram, and 4-gram precisions) to measure
the precision. A value of 0 means that the generated sentence
has no overlap with the reference while a value of 100 means
perfect overlap with the reference. Mathematically, the n-gram
precision pn is defined as:
P
P
C∈{ Candidates }
n -gram ∈C Count clip (n -gram )
P
pn = P
0
C 0 ∈{ Candidates }
n -gram ∈C 0 Count (n -gram )
(1)
BLEU combines all n-gram precision scores using geometric
mean:
N
X
ωn log pn
(2)
BLEU = BP · exp
n=1

ωn is the unifrom weight 1/N. The straightforward calculation
will result in high scores for short sentences or sentences with
repeated high-frequency n-grams. Therefore, Brevity Penalty
(BP) is used to scale the score and each n-gram in the reference
is limited to be used just once.
The original BLEU was designed for the corpus-level calculation [23]. Therefore, it does not need to be smoothed as
p4 is non-zero as long as there is at least one 4-gram match.
For sentence-level BLEU, since the generated sentences and
references are much shorter, p4 is more likely to be zero
when the sentence has no 4-gram or 4-gram match. Then the
geometric mean will be zero even if p1 , p2 , and p3 are large.
In this case, the score correlates poorly with human judgment.
Therefore, several smoothing methods are proposed [41] to
mitigate this problem.
There is an interpretation [42] of BLEU scores by Google,
which is shown in Table I. We also show the original BLEU
scores reported by existing approaches in Table II. These
scores vary a lot. Specifically, 19.61 for Astattgru would be
interpreted as “hard to get the gist” and 38.17 for Deepcom
would be interpreted as “understandable to good translations”
according to Table I. However, this interpretation is contrary
to the results shown in [14] where Astattgru is relatively
better than Deepcom. To study this issue, we need to explore
the difference and comparability of different metrics and
experimental settings used in different methods.

30-40
40-50
50-60
>60

TABLE II
T HE BEST BLEU SCORES REPORTED IN THE CORRESPONDING PAPERS .

Model

Score

CodeNN [9]
Deepcom [10]
Astattgru [14]
Rencos [15]
NCS [16]

20.50
38.17
19.61
20.70
44.14

III. E XPERIMENTAL D ESIGN
A. Datasets
We conduct the experiments on three widely used Java
datasets: TL-CodeSum [11], Funcom [14], and CodeSearchNet [18].
TL-CodeSum has 87,136 method-summary pairs crawled
from 9,732 Java projects created from 2015 to 2016 with at
least 20 stars. The ratio of the training, validation, and test
sets is 8:1:1. Since all pairs are shuffled, there can be methods
from the same project in the training, validation, and test sets.
In addition, there are exact code duplicates among the three
partitions.
CodeSearchNet is a well-formatted dataset containing
496,688 Java methods across the training, validation, and test
sets. Duplicates are removed and the dataset is split into
training, validation, and test sets in proportion with 8:1:1 by
project (80% of projects into training, 10% into validation, and
10% into testing) such that code from the same repository can
only exist in one partition.
Funcom is a collection of 2.1 million method-summary
pairs from 28,945 projects. Auto-generated code and exact
duplicates are removed. Then the dataset is split into three
parts for training, validation, and testing with the ratio of
9:0.5:0.5 by project.
For a systematic evaluation, we modify some characteristics
of the datasets (such as dataset size, deduplication, etc) and
obtain 9 new variants. In total, we experiment on 12 datasets,
as shown in Table III the statistics. In this paper, we use TLC,
FCM, and CSN to denote TL-CodeSum, Funcom, and CodeSearchNet, respectively. TLC is the original TL-CodeSum.

CSN and FCM are CodeSearchNet and Funcom with source
code that cannot be parsed by javalang1 filtered out. These
datasets are mainly different from each other in corpus sizes,
data splitting ways, and duplication ratios. For corpus sizes, we
set three magnitudes: small (the same size as TLC), medium
(the same size as CSN), and large (the same size as FCM).
Detailed descriptions of data splitting way and duplication can
be found in Section III-D.
B. Evaluated Approaches
We describe the code summarization models used in this
study:
• CodeNN [9] is the first neural approach that learns to
generate summaries of code snippets. It is a classical
encoder-decoder framework in NMT that encodes code
to context vectors with the attention mechanism and then
generates summaries in the decoder.
• Deepcom [10] is an SBT-based (Structure-based Traversal) model, which is more capable of learning syntactic
and structure information of Java methods.
• Astattgru [14] is a multi-encoder neural model that
encodes both code and AST to learn lexical and syntactic
information of Java methods.
• NCS [16] models code using Transformer to capture the
long-range dependencies, and it incorporates the copying
mechanism [43] in the Transformer to allow both generating words from vocabulary and copying from the input
source code.
• Rencos [15] enhances the neural model with the most
similar code snippets retrieved from the training set.
Therefore, it leverages both neural and retrieval-based
techniques.

D. Research Questions
This study investigates three research questions from three
aspects: metrics, pre-processing operations, and datasets.
RQ1: How do different evaluation metrics affect the
performance of code summarization?
There are several metrics commonly used for various NLP
tasks such as machine translation, text summarization, and
captioning. These metrics include BLEU [23], Meteor [44],
Rouge-L [45], Cider [46], etc. In RQ1, we only present
BLEU as it is the most commonly used metric in the code
summarization task. For other RQs, all metrics are calculated
(some of the results are put into Appendix due to space
limitation). As stated in Section II-B, BLEU can be calculated
at different levels and with smoothing methods. There are
many BLEU variants used in prior work and they could
generate different results for the same generated summary.
Here, we use the names of BLEU variants defined in [27]
and add another BLEU variant: BLEU-DM, which is the
Sentence BLEU without smoothing [41] and is based on the
implementation of NLTK3.2.4 . The meaning of these BLEU
variants are:
•

•
•

•
•

C. Experimental Settings
We use the default hyper-parameter settings provided by
each method and adjust the embedding size, hidden size,
learning rate, and max epoch empirically to ensure that each
model performs well on each dataset. We adopt max epoch
200 for TLC and TLCDedup (others are 40) and early stopping
with patience 20 to enable the convergence and generalization.
In addition, we run each experiment 3 times and display the
mean and standard deviation in the form of mean ± std. All
experiments are conducted on a machine with 252 GB main
memory and 4 Tesla V100 32GB GPUs.
We use the provided implementations by each approach:
CodeNN2 , Astattgru3 , NCS4 and Rencos5 . For Deepcom, we
re-implement the method6 according to the paper description
since it is not publicly available. We have checked the correctness by both reproducing the scores in the original paper [10]
and double confirmed with the authors of Deepcom.
1 https://github.com/c2nes/javalang
2 https://github.com/sriniiyer/codenn
3 https://bit.ly/2MLSxFg

•

BLEU-CN: This is a Sentence BLEU metric used in [9],
[19], [21]. It applies a Laplace-like smoothing by adding
1 to both the numerator and denominator of pn for n ≥ 2.
BLEU-DM: This is a Sentence BLEU metric used in [10],
[13]. It uses smoothing method0 based on NLTK3.2.4 .
BLEU-DC: This is a Sentence BLEU metric based on
NLTK3.2.4 method4 shared in the public code of HDeepcom7 .
BLEU-FC: This is an unsmoothed Corpus BLEU metric
based on NLTK, used in [14], [17], [22].
BLEU-NCS: This is a Sentence BLEU metric used in
[16]. It applies a Laplace-like smoothing by adding 1 to
both the numerator and denominator of all pn .
BLEU-RC: This is an unsmoothed Sentence BLEU metric used in [15]. To avoid the divided-by-zero error, it
adds a tiny number 10−15 in the numerator and a small
number 10−9 in the denominator of pn .

We first train and test the five approaches on TLC and
TLCDedup , and measure their generated summaries using different BLEU variants. Then we will introduce the differences
of the BLEU variants in detail, and summarize the reasons for
the differences from three aspects: different calculation levels
(sentence-level v.s. corpus-level), different smoothing methods used, and many problematic software implementations.
Finally, we analyze the impact of each aspect and provide
actionable guidance on the use of BLEU, such as how to
choose a smoothing method, what problematic implementations should be avoided, and how to report the BLEU scores
more clearly and comprehensively.
RQ2: How do different pre-processing operations affect
the performance of code summarization?

4 https://github.com/wasiahmad/NeuralCodeSum
5 https://github.com/zhangj111/rencos
6 The

code for our re-implementation is included in the anonymous link.

7 In fact, the scores displayed in the papers ( [10], [13]) were calculated by
BLEU-DM.

TABLE III
T HE STATISTICS OF THE 12 DATASETS USED .

#Method

Name

#Class

Training

Validation

Test

TLC
TLCDedup

69,708
69,708

8,714
8,714

8,714
6,449

CSN
CSNProject-Medium
CSNClass-Medium
CSNMethod-Medium
CSNMethod-Small

454,044
454,044
448,780
447,019
69,708

15,299
15,299
19,716
19,867
19,867

26,897
26,897
28,192
29,802
29,802

FCM
FCMProject-Large
FCMMethod-Large
FCMMethod-Medium
FCMMethod-Small

1,908,694
1,908,694
1,908,694
454,044
69,708

104,948
104,948
104,948
104,948
104,948

104,777
104,777
104,777
104,777
104,777

There are various code pre-processing operations used in
related work, such as token splitting, replacement, lowercase, filtering, etc. We select four operations that are widely
used [10], [11], [13]–[17], [20], [22] to investigate whether
different pre-processing operations would affect performance.
The four operations are:
• R: replace string and number with generic symbol
<STRING> and <NUM>.
• S: split tokens using camelCase and snake case.
• F : filter the punctuations in code.
• L: lowercase all tokens.
We define a bit-wise notation PRSF L to denote different
pre-processing combinations. For example, P1010 means R =
T rue, S = F alse, F = T rue, and L = F alse, which stands
for performing R, F , and preventing S, L. Then, we evaluate
different pre-processing combinations on TLCDedup dataset in
Section IV-B.
RQ3: How do different datasets affect the performance?
Many datasets have been used in source code summarization. We first evaluate the performance of different methods
on three widely used datasets, which are different in three
attributes: corpus size, data splitting methods, and duplication
ratio. Then, we study the impact of the three attributes with
the extended datasets shown in Table III. The three attributes
we consider are as follows:
1) Data splitting methods: there are three data splitting
ways we investigate: 1 by method: randomly split the
dataset after shuffling the all samples [11], 2 by class:
randomly divide the classes into the three partitions such
that code from the same class can only exist in one
partition, and 3 by project: randomly divide the projects
into the three partitions such that code from the same
project can only exist in one partition [14], [18].
2) Corpus sizes: there are three magnitudes of training set
size we investigate: 1 small: the training size of TLC,

–
–
136,495
136,495
136,495
136,495
–
–
–
–
–
–

#Project

Description

9,732
–

Original TL-CodeSum [11]
Deduplicated TL-CodeSum

25,596
25,596
25,596
25,596
–

Filtered CodeSearchNet [18]
CSN split by project
CSN split by class
CSN split by method
Subset of CSNMethod-Medium

28,790
28,790
28,790
–
–

Filtered Funcom [14]
Split FCM by project
Split FCM by method
Subset of FCMMethod-Large
Subset of FCMMethod-Large

2 medium: the training size of CSN, and 3 large(the
training size of FCM).
3) Duplication ratios: Code duplication is common in software development practice. This is often because developers copy and paste code snippets and source files
from other projects [47]. According to a large-scale
study [48], more than 50% of files were reused in more
than one open-source project. Normally, for evaluating
neural network models, the training set should not contain samples in the test set. Thus, ignoring code duplication may result in model performance and generalization
ability not being comprehensively evaluated according
to the actual practice. Among the three datasets we
experimented on, Funcom and CodeSearchNet contain
no duplicates because they have been deduplicated, but
we find the existence of 20% exact code duplication
in TL-CodeSum. Therefore, we conduct experiments on
TL-CodeSum with different duplication ratios to study
this effect.
IV. E XPERIMENTAL R ESULTS
A. How do different evaluation metrics affect the performance
of code summarization? (RQ1)
We experiment on the five approaches and measure their
generated summaries using different BLEU variants. The
results are shown in Table IV. We can find that:
• The scores of different BLEU variants are different for
the same summary. For example, the BLEU scores of
Deepcom on TLC vary from 12.14 to 40.18. Astattgru is
better than Deepcom in all BLEU variants.
• The ranking of models is not consistent using different
BLEU variants. For example, the score of Astattgru is
higher than that of CodeNN in terms of BLEU-FC but
lower than that of CodeNN in other BLEU variants on
TLC.

TABLE IV
D IFFERENT METRIC SCORES IN TLC AND TLC D EDUP . U NDERLINED SCORES REFER TO THE METRIC USED IN THE CORRESPONDING PAPER .

TLC
Model

CodeNN
Deepcom
Astattgru
Rencos
NCS

TLCDedup

BLEU-DM BLEU-FC BLEU-DC BLEU-CN BLEU-NCS BLEU-RC BLEU-DM BLEU-FC BLEU-DC BLEU-CN BLEU-NCS BLEU-RC

s, m0

c, m0

s, m4

s, m2

s, ml

s, m0

s, m0

c, m0

s, m4

s, m2

s, ml

s, m0

51.98
40.18
50.87
58.64
57.08

26.04
12.14
27.11
41.01
36.89

36.50
24.46
35.77
47.78
45.97

33.07
21.18
31.98
46.75
45.19

33.78
22.26
32.64
47.17
45.51

26.32
13.74
25.87
40.39
38.37

40.95
34.81
38.41
45.69
43.91

8.90
4.03
7.50
22.98
18.37

20.51
15.87
18.51
31.22
29.07

15.64
11.26
13.35
29.81
27.99

16.60
12.68
14.24
30.37
28.42

7.24
3.51
5.53
21.39
18.94

s and c represent sentence BLEU and corpus BLEU, respectively. mx represents different smoothing methods,
m0 is without smoothing method, and ml means using add-one Laplace smoothing which is similar to m2 .

•

Under the BLEU-FC measure, many existing models
(except Rencos) have scores lower than 20 on TLCDedup
dataset. According to the interpretations in Table I, this
means that under this experimental setting, the generated
summaries are not gist-clear and understandable.

Next, we elaborate on the differences among the BLEU
variants. The mathematical equation of BLEU is shown in
Equation (2), which combines all n-gram precision scores
using the geometric mean. The BP (Brevity Penalty) is used
to scale the score because the short sentence such as single
word outputs could potentially have high precision.
BLEU [23] is firstly designed for measuring the generated
corpus; as such, it requires no smoothing, as some sentences
would have at least one n-gram match. For sentence-level
BLEU, p4 will be zero when the example has not a 4-gram,
and thus the geometric mean will be zero even if pn (n < 4)
is large. For sentence-level measurement, it usually correlates
poorly with human judgment. Therefore, several smoothing
methods have been proposed in [41]. NLTK8 (the Natural
Language Toolkit), which is a popular toolkit with 9.7K stars,
implements the corpus-level and sentence-level measures with
different smoothing methods and are widely used in evaluating generated summaries [10], [11], [13], [14], [17], [20],
[22], [49]. However, there are problematic implementations
in different NLTK versions, leading to some BLEU variants
unusable. We further explain these differences in detail.
1) Sentence v.s. corpus BLEU: The BLEU score calculated
at the sentence level and corpus level is different, which is
mainly caused by the different calculation strategies for merging all sentences. The corpus-level BLEU treats all sentences
as a whole, where the numerator of pn is the sum of the
numerators of all sentences’ pn , and the denominator of pn
is the sum of the denominators of all sentences’ pn . Then
the final BLEU score is calculated by the geometric mean
of pn (n = 1, 2, 3, 4). Different from corpus-level BLEU,
sentence-level BLEU is calculated by separately calculating
the BLEU scores for all sentences, and then the arithmetic
average of them is used as sentence-level BLEU. In other
8 https://github.com/nltk/nltk

words, sentence-level BLEU aggregates the contributions of
each sentence equally, while for corpus-level, the contribution
of each sentence is positively correlated with the length of
the sentence. Because of the different calculation methods,
the scores of the two are not comparable. We thus suggest
explicitly report at which level the BLEU is being used.
2) Smoothing methods: Smoothing methods are applied
when deciding how to deal with cases if the number of
matched n-grams is 0. Since BLEU combines all n-gram
precision scores(pn ) using the geometric mean, BLEU will
be zero as long as any n-gram precision is zero. One may
add a small number to pn , however, it will result in the geometric mean is near zero. Thus, many smoothing methods are
proposed. Chen et al. [41] summarized 7 smoothing method.
Smoothing methods 1-4 replace 0 with a small positive value,
which can be a constant or a function of generated sentence
length. Smoothing methods 5-7 average the n − 1, n, and
n + 1–gram matched counts in different ways to obtain the ngram matched count. We plot the curve of pn under different
smoothing methods applied to sentences of varying lengths
in Fig. 1 (upper). We can find that values of pn calculated
by different smoothing methods can vary a lot, especially for
short sentences, which is the case for code summaries.
3) Bugs in software packages: We measure the same
summaries generated by CodeNN in three BLEU variants
(BLEU-DM, BLEU-FC, and BLEU-DC), which are all based
on NLTK implementation (but with different versions). From
Table V, we can observe that scores of BLEU-DM and BLEUDC are very different under different NLTK versions. This is
because the buggy implementations for method0 and method4
in different versions and buggy implementation can cause up
to 97% performance difference for the same metric under
different versions.
Smoothing method0 bug. method0 (means no smoothing
method) of NLTK3.2.x only combines the non-zero precision
values of all n-grams using the geometric mean. For example,
BLEU is the geometric mean of p1 , p2 , and p3 when p4 = 0
and pn 6= 0(n = 1, 2, 3).
Smoothing method4 bugs. method4 is implemented problematically in different NLTK versions. We plot the curve of pn

Different Smoothing Method

TABLE V
BLEU SCORES IN DIFFERENT NLTK VERSIONS .

NLTK version

Metric
3.2.x
BLEU-DM (s, m0 )
BLEU-FC (c, m0 )
BLEU-DC (s, m4 )

The length of generated sentence
Different Implementation of Method4

11

51.98
26.04
36.50

3.3.x/3.4.x

3.5.x

3.6.x12

26.32
26.04
36.50

26.32
26.04
42.39

26.32
26.04
28.35

Summary. The BLEU measure should be described precisely, including calculation level (sentence or corpus) and
smoothing method being used. Implementation correctness
should be carefully checked before use. Identified buggy
ones are: method0 in NLTK3.2.x and method4 from NLTK3.2.2
to NLTK3.5.x .
B. The effect of different pre-processing operations (RQ2)

The length of generated sentence

Fig. 1. Comparison on different smoothing methods.

of different smoothing method4 implementations in NLTK in
Fig. 1 bottom, where the correct version is NLTK3.6.x . In
1
NLTK versions 3.2.2 to 3.4.x, pn = n−1+C/ln(l
, where
h)
C = 5, which always inflates the score in different length
(Fig. 1). The correct method4 proposed in [41] is pn =
C
1/(invcnt ∗ ln(l
∗ lh ) , where C = 5 and invcnt = 21k
h)
is a geometric sequence starting from 1/2 to n-grams with
h)
0 matches. In NLTK3.5.x , pn = n−1+5/ln(l
where lh is the
lh
length of the generated sentence, thus pn can be assigned with
a percentage number that is much greater than 100% (even >
700%) when lh < 5 in n-gram. We have reported this issue9
and filed a pull request10 to NLTK GitHub repository, which
has been accepted and merged into the official NLTK library
and released in NLTK3.6.x (the revision is shown in Fig. 2).
Therefore, NLTK3.6.x should be used when using smoothing
method4 .
From the above experiments, we can conclude that BLEU
variants used in prior work on code summarization are different from each other and the differences can carry some risks
such as the validity of their claimed results. Thus, it is unfair
and risky to compare different models without using the same
BLEU implementation. For instance, it is unacceptable that
researchers ignore the differences among the BLEU variants
and directly compare their results with the BLEU scores
reported in other papers. We use the correct implementation
to calculate BLEU scores in the following experiments.

9 https://github.com/nltk/nltk/issues/2676
10 https://github.com/nltk/nltk/pull/2681

In order to evaluate the individual effect of four different code pre-processing operations and the effect of their
combinations, we train and test the four models (CodeNN,
Astattgru, Rencos, and NCS) under 16 different code preprocessing combinations. Note that the model Deepcom is not
experimented as it does not use source code directly. In the
following experiments, we have performed calculations on all
metrics. Due to space limitation, we present the scores under
BLEU-CN and BLEU-DC for RQ2 and BLEU-CN for RQ3.
All findings still hold for other metrics, and the omitted results
can be found in Appendix.
As shown in Table VI, we can observe that for all models,
performing S (identifier splitting) is always better than not
performing it, while it is not clear whether to perform the other
three operations. Then, we conduct the two-sided t-test [50]
and Wilcoxon-Mann-Whitney test [51] to statistically evaluate
the difference between using or dropping each operation. The
significance signs (*) labelled in Table VI mean that the pvalues of the statistical tests at 95% confidence level are less
than 0.05. The results confirm that the improvement achieved
by performing S is statistically significant, while performing
the other three operations does not lead to statistically different
results. The detailed statistical test scores can be found in
Appendix. As pointed out in [52], the OOV (out of vocabulary)
ratio is reduced after splitting compound words, and using
subtokens allows a model to suggest neologisms, which are
unseen in the training data. Many studies [53]–[57] have
shown that the performance of neural language models can
be improved after handling the OOV problem. Therefore,
the performance is improved after performing the identifier
splitting pre-processing.
Next, we evaluate the effect of different combinations of
the four code pre-processing operations and show the result in
11 Except for versions 3.2 and 3.2.1, as these versions are buggy with the
ZeroDivisionError exception. Please refer to https://github.com/nltk/
nltk/issues/1458 for more details.
12 NLTK
3.6.x are the versions with the BLEU calculation bug fixed by us.

TABLE VI
THE RESULT OF FOUR CODE PRE - PROCESSING OPERATIONS . 1 AND 0 DENOTES USE AND NON - USE OF A CERTAIN OPERATION , RESPECTIVELY.
MEAN STATISTICALLY SIGNIFICANT.

BLEU-CN

Model
CodeNN
Astattgru
Rencos
NCS
Avg.

R0
13.50
12.66
27.6
19.52
18.32

R1
13.51
12.64
27.37
19.43
18.24

S0
13.39
12.13
26.57
18.8
17.72

S1
13.62
13.17
28.41
20.15
18.84*

F0
13.51
12.55
27.53
19.37
18.24

S TARS *

BLEU-DC
F1
13.51
12.75
27.44
19.57
18.32

L0
13.52
12.50
27.24
19.16
18.10

L1
13.49
12.80
27.73
19.79
18.45

R0
7.19
5.91
21.85
12.20
11.79

R1
7.18
5.97
21.55
12.08
11.70

S0
S1
F0
7.18 7.19 7.18
5.63 6.26 5.85
20.91 22.5 21.79
11.65 12.63 12.04
11.34 12.15* 11.72

F1
7.19
6.03
21.62
12.24
11.77

L0
7.19
5.81
21.43
11.82
11.56

L1
7.18
6.07
21.98
12.45
11.92

TABLE VII
P ERFORMANCE OF DIFFERENT CODE PRE - PROCESSING COMBINATIONS , EVALUATED WITH BLEU-CN. (F OR EACH MODEL , THE TOP 5 SCORES ARE
MARKED IN RED AND THE BOTTOM 5 IN BLUE .)
Model

P0000

CodeNN
Astattgru
Rencos
NCS

13.21(6.31% ↓)
11.98(13.00% ↓)
26.04(13.14% ↓)
18.47(12.55% ↓)

P0001 P0010 P0011 P0100 P0101 P0110 P0111 P1000 P1001 P1010 P1011 P1100
13.29
12.40
26.08
19.09

13.21
12.08
26.93
18.23

13.36
11.98
26.61
19.11

14.10
12.74
27.54
19.62

13.44
13.77
29.20
20.83

13.84
13.10
28.16
19.65

13.64
13.09
28.40
20.42

13.36
12.25
26.59
19.06

13.41
12.43
26.64
18.68

13.46
11.99
26.78
18.97

13.83
11.92
26.87
18.76

13.53
12.74
27.44
19.70

P1101
13.71(3.79% ↑)
13.68(14.77% ↑)
29.98(15.13% ↑)
21.11(15.80% ↑)

P1110 P1111
13.47
13.12
28.46
19.56

13.23
13.13
28.06
20.30

TABLE VIII
P ERFORMANCE OF DIFFERENT CODE PRE - PROCESSING COMBINATIONS , EVALUATED WITH BLEU-DC.
Model

P0000

P0001 P0010 P0011 P0100 P0101 P0110 P0111 P1000 P1001 P1010 P1011 P1100

P1101

CodeNN 7.06(6.37% ↓) 7.10 6.98 7.25 7.54 7.01 7.43 7.06 7.22 7.19 7.24 7.40 7.06 7.34(5.16% ↑)
Astattgru 5.67(14.99% ↓) 5.65 5.44 5.48 6.17 6.67 6.28 6.41 5.84 5.83 5.30 5.81 5.79 6.62(24.91% ↑)
Rencos 20.21(16.52% ↓) 20.35 21.28 21.01 21.52 23.37 22.25 22.45 20.91 20.96 21.20 21.33 21.42 24.21(19.79% ↑)
NCS 11.22(17.92% ↓) 11.95 11.12 12.07 12.06 13.30 12.12 12.82 11.87 11.51 11.78 11.64 12.34 13.67(22.93% ↑)

Table VII and Table VIII. For each model, we mark the top 5
scores in red and the bottom 5 scores in blue. From Table VII
we can find that:
• Different pre-processing operations can affect the overall
performance by a noticeable margin.
• P1101 is a recommended code pre-processing method, as
it is in the top 5 for all approaches.
• P0000 is the not-recommended code pre-processing
method, as it is in the bottom 5 for all approaches.
• Generally, the ranking of performance for different models are generally consistent under different code preprocessing settings.
Summary. To choose the best pre-processing operations,
different combinations should be tested as different models
prefer different pre-processing and the difference can be
large (from -18% to +25%). Among them, using S (identifier
splitting) and P1101 is recommended, while P0000 is not
recommended.
C. How do different datasets affect the performance? (RQ3)
To answer RQ3, we evaluate the five approaches on the
three base datasets: TLC, CSN, and FCM. From Table IX, we
can find that:
• The performance of the same model is different on
different datasets.

P1110 P1111
7.02 7.05
6.03 6.09
22.62 22.15
12.09 12.67

nltk/translate/bleu_score.py
571

571
…
579

579

580

580

581

581
582
583
584
585

582
583
584
585
586
587
588
589
586

590

def method4(self, p_n, references, hypothesis,
hyp_len=None, *args, **kwargs):
...
+
incvnt = 1
hyp_len = hyp_len if hyp_len else len(hypothesis)
+
+
+
+
+
+
+
+

#
#
#
#

for i, p_i in enumerate(p_n):
if p_i.numerator == 0 and hyp_len != 0:
incvnt = i + 1 * self.k / math.log(
hyp_len
) # Note that this K is ...
p_n[i] = incvnt / p_i.denominator
if p_i.numerator == 0 and hyp_len >1:
incvnt = i + 1 * self.k / math.log(
hyp_len
) # Note that this K is ...
p_n[i] = incvnt / p_i.denominator\
numerator = 1 / (2 ** incvnt * self.k
/ math.log(hyp_len))
p_n[i] = numerator / p_i.denominator
incvnt += 1
return p_n

Fig. 2. Issue 26769 about smoothing method4 in NLTK, which is reported
and fixed by us.

•

The ranking among the approaches does not preserve
when evaluating them on different datasets. For instance,
Rencos outperforms other approaches in TLC but is
worse than Astattgru and NCS in the other two datasets.

TABLE IX
P ERFORMANCE ON DIFFERENT DATASETS .

Dataset

Model

•

•

TLC

FCM

CSN

CodeNN
Deepcom
Astattgru
Rencos
NCS

33.03±0.20
20.54±2.57
30.19±0.86
46.81±0.06
44.25±0.21

25.26±0.01
20.80±0.02
27.63±0.24
25.82±0.00
30.69±0.12

8.58±0.15
6.12±0.64
11.73±0.41
11.19±0.09
11.80±0.94

Avg

34.96±9.99

24.91±5.19

10.04±2.43

CodeNN performs better than Astattgru on TLC, but
Astattgru outperforms CodeNN in the other two datasets.
The average performance of all models on TLC is better
than the other two datasets, although TLC is much
smaller (about 96% less than FCM and 84% less than
CSN).
The average performance of FCM is better than that of
CSN.

Summary. To more comprehensively evaluate different
models, it is recommended to use multiple datasets, as the
ranking of model can be inconsistent on different datasets.
Since there are many factors that make the three datasets
different, in order to further explore the reasons for the above
results in-depth, we use the controlled variable method to
study from three aspects: corpus size, data splitting way, and
duplication ratio.
1) The impact of different corpus sizes: We evaluate all
models on two groups (one group contains CSNMethod-Medium
and CSNMethod-Small , the other group contains FCMMethod-Large ,
FCMMethod-Medium and FCMMethod-Small ). Within each group, the
test sets are the same, the only difference is in the corpus size.
The results are shown in Table X. We can find that the
ranking between models can be preserved on different corpus
sizes. Also, as the size of the training set becomes larger, the
performance of the five approaches improves in both groups,
which is consistent with the findings of previous work [19]. We
can also find that, compared to other models, the performance
of Deepcom does not improve significantly when the size of
the training set increases. We suspect that this is due to the
high OOV ratio, which affects the scalability of the Deepcom
model [52], [58], as shown in the bottom of Table X. Deepcom
uses only SBT and represents an AST node as a concatenation
of the type and value of the AST node, resulting in a sparse
vocabulary. Therefore, even if the training set becomes larger,
the OOV ratio is still high. Therefore, Deepcom could not
fully leverage the larger datasets.
Summary. If additional data is available, one can enhance
the performance of models by training with more data since
the performance improves as the size of the training set
becomes larger.

2) The impact of different data splitting ways: In this experiment, we evaluate the five approaches on two groups (one
group contains FCMProject-Large and FCMMethod-Large and another
contains CSNProject-Medium , CSNClass-Medium , CSNMethod-Medium ).
Each group only differs in data splitting ways. From Table XI,
we can observe that all approaches perform differently in
different data splitting ways, and they all perform better on
the dataset split by method than by project. This is because
similar tokens and code patterns are used in the methods from
the same project [59]–[61]. In addition, when the data splitting
ways are different, the rankings between various approaches
remain basically unchanged, which indicates that it would
not impact comparison fairness across different approaches
whether or not to consider multiple data-splitting ways.
Summary. Different data splitting ways will significantly
affect the independent performance of all models. However,
the ranking of the model remains basically unchanged.
Therefore, if data availability or time is limited, it is also
reliable to evaluate the performance of different models
under only one data splitting way.
3) The impact of different duplication ratios: To simulate
scenarios with different code duplication ratios, we construct
synthetic test sets from TLCDedup by adding random samples
from the training set to the test set. Then, we train the five
models using the same training set and test them using the
synthetic test sets with different duplication ratios (i.e., the
test sets with random samples). From the results shown in
Fig. 3, we can find that:
• The BLEU scores of all approaches increase as the
duplication ratio increases.
• The score of the model Rencos increases significantly
when the duplication ratio increases. We speculate that
the reason should be the duplicated samples being retrieved back by the retrieval module in Rencos. Therefore,
retrieval-based models could benefit more from code
duplication.
• In addition, the ranking of the models is not preserved
with different duplication ratios. For instance, CodeNN
outperforms Astattgru without duplication and is no better
than Astattgru on other duplication ratios.
Summary. To evaluate the performance of neural code
summarization models, it is recommended to use deduplicated datasets so that the generalization ability of the model
itself can be tested. However, in real scenarios, duplications
are natural. Therefore, we suggest evaluating models under
different duplication ratios. Moreover, it is recommended to
consider incorporating retrieval techniques to improve the
performance especially when code duplications exist.
V. T HREATS TO VALIDITY
We have identified the following main threats to validity:
• Programming languages. We only conduct experiments
on Java datasets. Although, in principle, the models and

TABLE X
T HE RESULT OF DIFFERENT CORPUS SIZES .

Model

FCMMethod-Small

FCMMethod-Medium

FCMMethod-Large

CSNMethod-Small

CSNMethod-Medium

CodeNN
Deepcom
Astattgru
Rencos
NCS

22.85±0.12
20.49±0.16
23.94±0.67
24.02±0.03
27.89±0.37

27.38±0.04
22.78±0.12
29.83±0.20
31.47±0.04
35.41±0.20

31.19±0.17
23.72±0.32
33.36±0.16
33.95±0.03
40.73±0.16

9.38±0.14
12.30±0.64
11.38±0.42
11.73±0.16
12.74±0.13

20.13±0.34
12.64±1.07
24.11±0.25
25.03±0.02
30.12±0.27

OOV Ratio of Deepcom
OOV Ratio of Others

91.90%
63.36%

88.94%
53.09%

88.32%
48.60%

91.49%
60.99%

85.81%
34.00%

BLEU

TABLE XI
T HE RESULT OF DIFFERENT DATA SPLITTING WAYS .

100
80
60
40
20
0

Model

CSNProject-Medium

CSNClass-Medium

CSNMethod-Medium

FCMProject-Large

FCMMethod-Large

CodeNN
Deepcom
Astattgru
Rencos
NCS

8.58±0.15
6.12±0.64
11.73±0.41
11.19±0.09
11.80±0.94

16.16±0.20
11.29±0.21
20.22±0.39
19.75±0.10
23.25±0.13

20.13±0.34
12.64±1.07
24.11±0.25
25.03±0.02
30.12±0.27

25.26±0.00
20.80±0.02
27.63±0.24
25.82±0.00
30.69±0.12

31.19±0.17
23.72±0.32
33.36±0.16
33.95±0.03
40.73±0.16

OOV Ratio

48.74%

35.38%

34.00%

57.56%

48.60%

CodeNN
Deepcom
Astattgru
Rencos
NCS
•

0

0.1
0.3
0.5
0.7
Different duplication ratio

0.9

Fig. 3. The result of different duplication ratios.

•

•

•

experiments are not specifically designed for Java, more
evaluations are needed when generalizing our findings to
other languages. In the future, we will extend our study
to other programming languages.
The quality of summaries. The summaries in all datasets
are collected by extracting the first sentences of Javadoc.
Although this is a common practice to place a method’s
summary at the first sentence according to the Javadoc
guidelines13 , there might still be some incomplete or
mismatched summaries in the datasets.
Data difference. We observe that even when we control
all three factors (splitting methods, duplication ratios, and
dataset sizes), the performance of the same model still
varies greatly between different datasets14 . This indicates
that the differences in training data may also be a factor
that affects the performance of code summarization. We
leave it to future work to study the impact of data
differences.
Models evaluated. We covered all representative models
with different characteristics, such as both RNN-based
and Transformer-based models, both single-channel and

13 http://www.oracle.com/technetwork/articles/java/index-137868.html
14 The

result is put into Appendix due to space limitation.

multi-channel models, both models with and without
retrieval techniques. However, other models that we are
out of our study may still cause our findings to be
untenable.
Human evaluation. We use quantitative evaluation metrics
to evaluate the code summarization results. Although
these metrics are used in almost all related work, qualitative human evaluation can further confirm the validity
of our findings. We defer a thorough human evaluation
to future work.
VI. C ONCLUSION

In this paper, we conduct an in-depth analysis of recent
neural code summarization models. We have investigated several aspects of model evaluation: evaluation metrics, datasets,
and code pre-processing operations. Our results point out that
all these aspects have a large impact on evaluation results.
Without a carefully and systematically designed experiment,
neural code summarization models cannot be fairly evaluated and compared. Our work also suggests some actionable
guidelines including: (1) using proper (and maybe multiple)
code pre-processing operations (2) selecting and reporting
BLEU metrics explicitly (including a sentence or corpus level,
smoothing method, NLTK version, etc) (3) considering the
dataset characteristics when evaluating and choosing the best
model. We believe the results and findings we obtained can
be of great help for practitioners and researchers working on
this interesting area.
In our future work, we will extend our study to programming languages other than Java. We will also explore more
important attributes of dataset and investigate better techniques
for building a higher-quality parallel corpus. Furthermore,
we plan to extend our guidelines actionable to other text
generation tasks in software engineering such as commit
message generation.

R EFERENCES
[1] S. R. Tilley, H. A. Müller, and M. A. Orgun, “Documenting software
systems with views,” in SIGDOC. ACM, 1992, pp. 211–219.
[2] L. C. Briand, “Software documentation: How much is enough?” in
CSMR. IEEE Computer Society, 2003, p. 13.
[3] A. Forward and T. Lethbridge, “The relevance of software documentation, tools and technologies: a survey,” in ACM Symposium on Document
Engineering. ACM, 2002, pp. 26–33.
[4] G. Sridhara, E. Hill, D. Muppaneni, L. L. Pollock, and K. VijayShanker, “Towards automatically generating summary comments for java
methods,” in ASE, 2010, pp. 43–52.
[5] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. L. Pollock, and
K. Vijay-Shanker, “Automatic generation of natural language summaries
for java classes,” in ICPC. IEEE Computer Society, 2013, pp. 23–32.
[6] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccfinder: A multilinguistic
token-based code clone detection system for large scale source code,”
IEEE Trans. Software Eng., vol. 28, no. 7, pp. 654–670, 2002.
[7] M. Kim, V. Sazawal, D. Notkin, and G. C. Murphy, “An empirical study
of code clone genealogies,” in ESEC/SIGSOFT FSE. ACM, 2005, pp.
187–196.
[8] Z. Li, S. Lu, S. Myagmar, and Y. Zhou, “Cp-miner: Finding copy-paste
and related bugs in large-scale software code,” IEEE Trans. Software
Eng., vol. 32, no. 3, pp. 176–192, 2006.
[9] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing source
code using a neural attention model,” in ACL, 2016.
[10] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment
generation,” in ICPC, 2018.
[11] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, “Summarizing source
code with transferred api knowledge,” in IJCAI, 2018.
[12] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, “Improving automatic source code summarization via deep reinforcement
learning,” in ASE, 2018.
[13] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation
with hybrid lexical and syntactical information,” Empirical Software
Engineering, 2019.
[14] A. LeClair, S. Jiang, and C. McMillan, “A neural model for generating
natural language summaries of program subroutines,” in ICSE, 2019.
[15] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, “Retrieval-based
neural source code summarization,” in ICSE, 2020.
[16] W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, “A transformerbased approach for source code summarization,” in ACL, 2020.
[17] B. Wei, Y. Li, G. Li, X. Xia, and Z. Jin, “Retrieve and refine: Exemplarbased neural comment generation,” in ASE. IEEE, 2020, pp. 349–360.
[18] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code
search,” arXiv Preprint, 2019. [Online]. Available: https://arxiv.org/abs/
1909.09436
[19] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating
sequences from structured representations of code,” in ICLR (Poster).
OpenReview.net, 2019.
[20] B. Wei, G. Li, X. Xia, Z. Fu, and Z. Jin, “Code generation as a dual
task of code summarization,” in NeurIPS, 2019, pp. 6559–6569.
[21] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained model for
programming and natural languages,” in EMNLP (Findings), 2020.
[22] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in ICPC. ACM, 2020,
pp. 184–195.
[23] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in ACL. ACL, 2002, pp.
311–318.
[24] N. Nazar, Y. Hu, and H. Jiang, “Summarizing software artifacts: A
literature review,” Journal of Computer Science and Technology, vol. 31,
no. 5, pp. 883–909, 2016.
[25] X. Song, H. Sun, X. Wang, and J. Yan, “A survey of automatic generation
of source code comments: Algorithms and techniques,” IEEE Access,
vol. 7, pp. 111 411–111 428, 2019.
[26] Y. Zhu and M. Pan, “Automatic code summarization: A systematic
literature review,” arXiv preprint arXiv:1909.04352, 2019.
[27] D. Gros, H. Sezhiyan, P. Devanbu, and Z. Yu, “Code to comment
”translation”: Data, metrics, baselining & evaluation,” in ASE, 2020.

[28] S. Haiduc, J. Aponte, and A. Marcus, “Supporting program comprehension with source code summarization,” in ICSE, vol. 2. ACM, 2010,
pp. 223–226.
[29] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, “On the use of
automated text summarization techniques for summarizing source code,”
in WCRE. IEEE Computer Society, 2010, pp. 35–44.
[30] B. P. Eddy, J. A. Robinson, N. A. Kraft, and J. C. Carver, “Evaluating
source code summarization techniques: Replication and expansion,” in
ICPC. IEEE Computer Society, 2013, pp. 13–22.
[31] P. Rodeghero, C. McMillan, P. W. McBurney, N. Bosch, and S. K.
D’Mello, “Improving automated source code summarization via an eyetracking study of programmers,” in ICSE. ACM, 2014, pp. 390–401.
[32] P. Fernandes, M. Allamanis, and M. Brockschmidt, “Structured neural
summarization,” in ICLR, 2019.
[33] R. Cai, Z. Liang, B. Xu, Z. Li, Y. Hao, and Y. Chen, “Tag: Type auxiliary
guiding for code comment generation,” in ACL, 2020.
[34] A. Bansal, S. Haque, and C. McMillan, “Project-level encoding for
neural source code summarization of subroutines,” in ICPC, 2021.
[35] C. Lin, Z. Ouyang, J. Zhuang, J. Chen, H. Li, and R. Wu, “Improving
code summarization with block-wise abstract syntax tree splitting,” in
ICPC, 2021.
[36] R. Xie, W. Ye, J. Sun, and S. Zhang, “Exploiting method names
to improve code summarization: A deliberation multi-task learning
approach,” in ICPC, 2021.
[37] Q. Chen and M. Zhou, “A neural framework for retrieval and summarization of source code,” in ASE, 2018.
[38] S. Haque, A. LeClair, L. Wu, and C. McMillan, “Improved automatic
summarization of subroutines via attention to file context,” in MSR,
2020.
[39] W. Ye, R. Xie, J. Zhang, T. Hu, X. Wang, and S. Zhang, “Leveraging
code generation to improve code retrieval and summarization via dual
learning,” in The Web Conference, 2020.
[40] W. Wang, Y. Zhang, Y. Sui, Y. Wan, Z. Zhao, J. Wu, P. Yu, and
G. Xu, “Reinforcement-learning-guided source code summarization via
hierarchical attention,” IEEE Transactions on Software Engineering,
2020.
[41] B. Chen and C. Cherry, “A systematic comparison of smoothing techniques for sentence-level BLEU,” in WMT@ACL. The Association for
Computer Linguistics, 2014, pp. 362–367.
[42] G. Cloud, “Automl: Evaluating models,” 2007. [Online]. Available:
https://cloud.google.com/translate/automl/docs/evaluate#bleu
[43] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization
with pointer-generator networks,” in ACL, 2017.
[44] S. Banerjee and A. Lavie, “METEOR: an automatic metric for MT
evaluation with improved correlation with human judgments,” in IEEvaluation@ACL, 2005.
[45] C. Lin, “ROUGE: A package for automatic evaluation of summaries,”
in ACL, 2004.
[46] R. Vedantam, C. L. Zitnick, and D. Parikh, “Cider: Consensus-based
image description evaluation,” in CVPR, 2015.
[47] C. V. Lopes, P. Maj, P. Martins, V. Saini, D. Yang, J. Zitny, H. Sajnani,
and J. Vitek, “Déjàvu: a map of code duplicates on github,” in OOPSLA,
2017.
[48] A. Mockus, “Large-scale code reuse in open source software,” in First
International Workshop on Emerging Trends in FLOSS Research and
Development (FLOSS’07: ICSE Workshops 2007). IEEE, 2007, pp.
7–7.
[49] S. Stapleton, Y. Gambhir, A. LeClair, Z. Eberhart, W. Weimer, K. Leach,
and Y. Huang, “A human study of comprehension and code summarization,” in ICPC. ACM, 2020, pp. 2–13.
[50] S. Dowdy, S. Wearden, and D. Chilko, Statistics for research. John
Wiley & Sons, 2011, vol. 512.
[51] H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two
Random Variables is Stochastically Larger than the Other,” The Annals
of Mathematical Statistics, vol. 18, no. 1, pp. 50 – 60, 1947. [Online].
Available: https://doi.org/10.1214/aoms/1177730491
[52] R. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big
code != big vocabulary: open-vocabulary models for source code,” in
ICSE. ACM, 2020, pp. 1073–1085.
[53] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention
network for extreme summarization of source code,” in ICML, ser. JMLR
Workshop and Conference Proceedings, vol. 48. JMLR.org, 2016, pp.
2091–2100.

[54] E. Grave, A. Joulin, and N. Usunier, “Improving neural language models
with a continuous cache,” in ICLR (Poster). OpenReview.net, 2017.
[55] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” in ICLR (Poster). OpenReview.net, 2017.
[56] I. Bazzi, “Modelling out-of-vocabulary words for robust speech recognition,” Ph.D. dissertation, Massachusetts Institute of Technology, 2002.
[57] T. Luong, R. Socher, and C. D. Manning, “Better word representations
with recursive neural networks for morphology,” in CoNLL. ACL, 2013,
pp. 104–113.
[58] V. J. Hellendoorn and P. T. Devanbu, “Are deep neural networks the
best choice for modeling source code?” in FSE, 2017.
[59] S. Panthaplackel, P. Nie, M. Gligoric, J. J. Li, and R. J. Mooney,
“Learning to update natural language comments based on code changes,”
in ACL, 2020.
[60] A. LeClair and C. McMillan, “Recommendations for datasets for source
code summarization,” in NAACL, 2019.
[61] S. Liu, C. Gao, S. Chen, L. Y. Nie, and Y. Liu, “ATOM: commit message
generation based on abstract syntax tree and hybrid ranking,” arXiv,
2019.

